# DeepSeek-R1-Paper-Implementations
I am going to learn and code various components of the popular DeepSeek-R1 paper and share my findings using the NumPy library.

## Knowledge Distillation

### What is Knowledge Distillation
A technique used in machine learning to transfer knowledge from a large, complex model called the 'teacher' to a smaller, more efficient model called the 'student'. By doing this, the student is able to learn from the teacher's expertise without needing to replicate the full training process. This allows individuals to deploy lightweight models that maintain most of the model's accuracy, but are also faster and require less resources.
